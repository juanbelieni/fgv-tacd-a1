{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 – Linear Regression\n",
    "\n",
    "Use the dataset you loaded in Part 1 with the dataset splits from Part 2. You will implement two different solutions for linear regression with weight decay regularization:\n",
    "\n",
    "- Using the closed form (normal equations with weight decay)\n",
    "- Using stochastic gradient descent.\n",
    "\n",
    "Your implementation will predict the value of the “price” variable using all the remaining numerical features. It will consist of a scikit-learn estimator API with the following parameters:\n",
    "\n",
    "- **solver**: This parameter selects which algorithm is used to learn the coefficients of the linear regression. Passing “cf” will select the closed form solution, passing “sgd” will select the stochastic gradient descent. Set the default value to “cf”.\n",
    "- **max_iter**: This parameter is relevant only when the solver is “sgd”. It controls the number of iterations (over the entire dataset) of the stochastic gradient descent algorithm. Set the default value to 100.\n",
    "- **learning_rate**: This parameter is relevant only when the solver is “sgd”. Set the default value to 0.0001.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and splitting the dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "diamonds_dataset = pd.read_csv(\"../data/diamonds.csv\")\n",
    "diamonds_dataset = diamonds_dataset.drop(columns=[\"cut\", \"color\", \"clarity\"])\n",
    "\n",
    "n = len(diamonds_dataset)\n",
    "splits = [int(0.8 * n), int(0.9 * n)]\n",
    "\n",
    "training, validation, testing = np.split(diamonds_dataset.sample(frac=1), splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the linear regression model using\n",
    "# the closed form solution and the stochastic gradient descent\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class LinearRegression(BaseEstimator):\n",
    "    def __init__(\n",
    "        self, solver: str = \"sgd\", max_iter: int = 100, learning_rate: float = 0.0001\n",
    "    ):\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _fit_cf(self, X: np.ndarray, y: np.ndarray):\n",
    "        n = X.shape[0]\n",
    "        regularization_term = np.sqrt(np.sum(np.square(X)) / n)\n",
    "\n",
    "        lambda_I = np.eye(X.shape[1]) * regularization_term\n",
    "        w = np.linalg.inv(X.T @ X + lambda_I) @ X.T @ y\n",
    "\n",
    "        return w\n",
    "\n",
    "    def _fit_sgd(self, X: np.ndarray, y: np.ndarray):\n",
    "        n_samples, n_features = X.shape\n",
    "        w = np.zeros(n_features)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            for j in range(n_samples):\n",
    "                gradient = (y[j] - w @ X[j]) * X[j]\n",
    "                w += self.learning_rate * gradient\n",
    "\n",
    "        return w\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        n = X.shape[0]\n",
    "        X_ = np.c_[np.ones(n), X]\n",
    "\n",
    "        if self.solver == \"cf\":\n",
    "            self.w = self._fit_cf(X_, y)\n",
    "        elif self.solver == \"sgd\":\n",
    "            self.w = self._fit_sgd(X_, y)\n",
    "        else:\n",
    "            raise ValueError(\"Solver not implemented\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_ = np.c_[np.ones(len(X)), X]\n",
    "        return X_ @ self.w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "\n",
    "X_training = training.drop(columns=[\"price\"]).to_numpy()\n",
    "y_training = training[\"price\"].to_numpy()\n",
    "\n",
    "X_testing = testing.drop(columns=[\"price\"]).to_numpy()\n",
    "y_testing = testing[\"price\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating a \"dummy\" prediction and its\n",
    "# associated MSE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dummy_predictions = np.full(y_testing.shape[0], np.mean(y_training))\n",
    "dummy_error = mean_squared_error(y_testing, dummy_predictions, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:\t\t1510.894688869362\n",
      "Dummy mean squared error:\t4008.4226792098107\n",
      "Ratio:\t\t\t\t0.376929982136317\n"
     ]
    }
   ],
   "source": [
    "# Predicting the value of the price variable\n",
    "# using the closed form solution with weight\n",
    "# decay\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "cf_lr = LinearRegression(solver=\"cf\")\n",
    "\n",
    "cf_lr.fit(X_training, y_training)\n",
    "predictions = cf_lr.predict(X_testing)\n",
    "error = mean_squared_error(y_testing, predictions, squared=False)\n",
    "\n",
    "print(f\"Mean squared error:\\t\\t{error}\")\n",
    "print(f\"Dummy mean squared error:\\t{dummy_error}\")\n",
    "print(f\"Ratio:\\t\\t\\t\\t{error / dummy_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:\t\t1537.2902960096064\n",
      "Dummy mean squared error:\t4008.4226792098107\n",
      "Ratio:\t\t\t\t0.38351501801019044\n"
     ]
    }
   ],
   "source": [
    "# Predicting the value of the price variable\n",
    "# using the stochastic gradient descent\n",
    "\n",
    "sgd_lr = LinearRegression(solver=\"sgd\")\n",
    "\n",
    "sgd_lr.fit(X_training, y_training)\n",
    "predictions = sgd_lr.predict(X_testing)\n",
    "error = mean_squared_error(y_testing, predictions, squared=False)\n",
    "\n",
    "print(f\"Mean squared error:\\t\\t{error}\")\n",
    "print(f\"Dummy mean squared error:\\t{dummy_error}\")\n",
    "print(f\"Ratio:\\t\\t\\t\\t{error / dummy_error}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "328aee3b155a068886fab24c8a017cee24eb81c3e59bc834938456388e09f401"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
